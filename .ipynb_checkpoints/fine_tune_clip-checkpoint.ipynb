{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae98b3f9-76ef-421b-9aaf-f996860481c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snowcrash/.pyenv/versions/3.8.16/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a646b466-41f2-4029-b206-503a2a79bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame_2165.jpg', 'frame_0161.jpg', 'frame_0149.jpg', 'frame_1255.jpg', 'frame_2990.jpg', 'frame_0639.jpg', 'frame_0189.jpg', 'frame_0002.jpg', 'frame_0167.jpg', 'frame_0173.jpg', 'frame_1682.jpg', 'frame_2559.jpg', 'frame_1197.jpg', 'frame_1009.jpg', 'frame_0703.jpg', 'frame_1025.jpg', 'frame_0111.jpg', 'frame_1178.jpg', 'frame_2665.jpg', 'frame_2467.jpg', 'frame_0304.jpg', 'frame_1218.jpg', 'frame_2076.jpg', 'frame_1594.jpg', 'frame_0098.jpg', 'frame_2899.jpg', 'frame_0137.jpg', 'frame_2720.jpg', 'frame_1981.jpg', 'frame_2483.jpg', 'frame_1997.jpg', 'frame_0722.jpg', 'frame_2647.jpg', 'frame_0534.jpg', 'frame_0326.jpg', 'frame_0119.jpg', 'frame_0125.jpg', 'frame_0131.jpg', 'frame_2915.jpg', 'frame_2685.jpg', 'frame_0286.jpg', 'frame_0155.jpg', 'frame_0550.jpg', 'frame_2186.jpg', 'frame_0744.jpg', 'frame_2147.jpg', 'frame_2225.jpg', 'frame_0143.jpg', 'frame_0180.jpg', 'frame_1739.jpg', 'frame_0369.jpg', 'frame_1516.jpg', 'frame_0783.jpg', 'frame_1110.jpg', 'frame_1702.jpg', 'frame_1500.jpg', 'frame_0623.jpg', 'frame_1663.jpg']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'frame_0161.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrass\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#create an array of preprocessed images\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [preprocess(Image\u001b[38;5;241m.\u001b[39mopen(img)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_2165.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m#prediction with one prompt, several images. We need to stack the images together\u001b[39;00m\n\u001b[1;32m     12\u001b[0m  logits_per_image, logits_per_text \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mstack(imgs)\u001b[38;5;241m.\u001b[39mto(device),text)\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrass\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#create an array of preprocessed images\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [preprocess(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_2165.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m#prediction with one prompt, several images. We need to stack the images together\u001b[39;00m\n\u001b[1;32m     12\u001b[0m  logits_per_image, logits_per_text \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mstack(imgs)\u001b[38;5;241m.\u001b[39mto(device),text)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'frame_0161.jpg'"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "#my batch of images\n",
    "path = './frames'\n",
    "images = [f for f in os.listdir('./frames') if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "print(images)\n",
    "#what I am searching\n",
    "text = clip.tokenize([\"grass\"]).to(device)\n",
    "#create an array of preprocessed images\n",
    "imgs = [preprocess(Image.open(f\"{path}/{img}\")) for img in images if]\n",
    "with torch.no_grad():\n",
    "  #prediction with one prompt, several images. We need to stack the images together\n",
    " logits_per_image, logits_per_text = model(torch.stack(imgs).to(device),text)\n",
    " probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "print(\"Label probs:\", probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
